// This is an autogenerated file from Firebase Studio.

'use server';

/**
 * @fileOverview Dynamically selects and displays feature importance or SHAP values to explain model predictions.
 *
 * - explainablePredictions - A function that intelligently selects and displays feature importance or SHAP values.
 * - ExplainablePredictionsInput - The input type for the explainablePredictions function.
 * - ExplainablePredictionsOutput - The return type for the explainablePredictions function.
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';

const ExplainablePredictionsInputSchema = z.object({
  prediction: z.string().describe('The model prediction to be explained.'),
  features: z.record(z.number()).describe('The features used for the prediction.'),
  modelType: z.string().describe('The type of model used for the prediction (e.g., Random Forest, XGBoost, Neural Network).'),
});

export type ExplainablePredictionsInput = z.infer<typeof ExplainablePredictionsInputSchema>;

const ExplainablePredictionsOutputSchema = z.object({
  explanationType: z.enum(['feature_importance', 'shap_values']).describe('The type of explanation provided (feature importance or SHAP values).'),
  explanation: z.record(z.number()).describe('The feature importance or SHAP values for each feature.'),
  rationale: z.string().describe('The rationale for choosing the explanation type and the overall explanation.'),
});

export type ExplainablePredictionsOutput = z.infer<typeof ExplainablePredictionsOutputSchema>;

export async function explainablePredictions(input: ExplainablePredictionsInput): Promise<ExplainablePredictionsOutput> {
  return explainablePredictionsFlow(input);
}

const prompt = ai.definePrompt({
  name: 'explainablePredictionsPrompt',
  input: {schema: ExplainablePredictionsInputSchema},
  output: {schema: ExplainablePredictionsOutputSchema},
  prompt: `You are an AI assistant that explains model predictions by selecting the most appropriate method (feature importance or SHAP values) based on the model type and providing a rationale for the choice.

  Model Prediction: {{{prediction}}}
  Features: {{JSONstringify features}}
  Model Type: {{{modelType}}}

  Based on the model type and the nature of the features, determine whether feature importance or SHAP values would be a more appropriate explanation method. Explain your choice in the rationale.

  Provide the explanation in the following JSON format:
  {
    "explanationType": "feature_importance" | "shap_values",
    "explanation": { // Record<string, number> of feature importance or SHAP values for each feature }
    "rationale": "Explanation of why the chosen method was selected and the interpretation of the results."
  }`,
});

const explainablePredictionsFlow = ai.defineFlow(
  {
    name: 'explainablePredictionsFlow',
    inputSchema: ExplainablePredictionsInputSchema,
    outputSchema: ExplainablePredictionsOutputSchema,
  },
  async input => {
    const {output} = await prompt(input);
    return output!;
  }
);
